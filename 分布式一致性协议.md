# 分布式一致性协议

## 二阶段提交/三阶段提交

## Raft

## 拜占庭共识协议

### PBFT

- PBFT中view-change机制保证一致性的原理

    - view-change机制
        1. replica发送view-change消息，其中包含所有该replica达到prepared状态的请求（同时包含对这个prepared状态的证明，即2f个prepare消息）。
        2. next primary收到2f个replica的view-change消息，完成primary轮换。
        3. next primary对这2f个view-change消息（再加上自己的view-change消息）中包含的达到prepared状态的请求取并集，并对每个请求发送pre-prepare消息，开始共识。
    
    - 正确性证明
        1. prepared状态，锁定同一序号下的请求，即不存在同一序号下的两个请求都达到prepared状态。
        （详细证明：primary作恶的情况下，给f+1个non-faulty replica发送pre-prepare(a)，给其他f个non-faulty replica发送pre-prepare(b)，那么即使f-1个faulty replica发送prepare(b)消息，non-faulty replica也只会收到2f-1个prepare(b)，因此prepare阶段一定能锁定a）
        2. commited-local状态，即请求完成共识，节点会持久化其结果。
        3. 正确性就表示一个请求如果在某个non-faulty replica达到commited-local状态，这个请求在其他non-faulty replica最终也一定会达到commited-local状态。
        如果请求在一个non-faulty replica达到commited-local状态，就表示这个请求在2f+1个replica达到prepared状态，最坏情况下是f+1个non-faulty replica达到prepared状态，那么在view-change阶段，最坏情况下有f个faulty replica和f个没有达到prepared状态的non-faulty replica发送了不包含这个请求的view-change消息，但至少还有1个达到prepared状态的non-faulty replica发送了包含这个请求的view-change消息，因此这个请求一定会在view-change阶段被传递，最终完成共识。
        ![PBFT中的view-change机制1](E:/研究生课程/笔记/Resource/PBFT中的view-change机制1.png)

    - 其他情况的错误
        1. 不采用2PC（二阶段提交），去掉commit阶段，view-change消息记录达到prepared状态的请求。
        不采用2PC表示请求达到prepared状态就是完成了共识，那么有可能请求只在1个replica达到了prepared状态，而view-change阶段这个replica发送的view-change消息恰好没有被收到，那么之后的view中这个请求不会再被共识，导致不一致。
        ![PBFT中的view-change机制2](E:/研究生课程/笔记/Resource/PBFT中的view-change机制2.png)
        2. view-change消息记录pre-prepare消息。
        pre-prepare消息会存在不一致的情况（因为正确性证明中提到了，prepared状态才能锁定同一个序号下的请求，或者说，view-change阶段faulty replica恶意发送不一致的pre-prepare消息即可），这样收到view-change消息的next primary无法决定采用那个。
        ![PBFT中的view-change机制3](E:/研究生课程/笔记/Resource/PBFT中的view-change机制3.png)
        3. view-change消息记录达到commited-local状态的请求。
        这种情况与情况1类似。

- PBFT中的其他问题

    - 是否存在不同视图中，相同序号的请求，内容不同的情况？
    在多个view的主节点都是non-faulty replica的情况下，由于PBFT保证了在至少一个non-faulty replica处达到commited-local状态的请求会在view-change阶段被传递，相同序号的请求只可能内容相同或为空（即没有被传递）；
    在某个view的主节点是faulty replica的情况下，即使不遵从协议，想发送相同序号、不同内容的请求，也需要prepared状态的证明，这是无法做到的，因此相同序号的请求也只可能内容相同或为空。

    - pre-prepare、prepare、commit三个阶段中，某阶段消息缺失是否影响下阶段共识？
    prepare阶段和commit阶段互不影响，即没有收到足够的prepare消息，而收到足够的commit消息，也可以达到commited-local状态；
    pre-prepare阶段影响其他两个阶段，即prepare阶段和commit阶段若要完成（即达到prepared状态或commited-local状态），必须先收到对应的pre-prepare消息。
    **note：** PBFT中是这样描述的，而HoneyBadgerBFT中则不需要（echo阶段收集到了足够的消息，不需要收到val消息，就可以广播ready消息）。核心原因是PBFT中的第二个阶段（prepare阶段）消息中不包含请求内容，对于某个在至少一个non-faulty replica处达到commited-local状态的请求，如果发生view-change，这个请求至少在f+1个non-faulty replica处达到prepared状态，但这f+1个non-faulty replica如果没有收到pre-prepare消息就达到prepared状态，就表示不知道请求内容，会导致一致性存在问题（PBFT的目标是f+1个以上的non-faulty replica完成共识）；而HoneyBadgerBFT中的第二个阶段（echo阶段）消息包含请求内容（merkle tree形式），因此不需要收到val消息。

    - view-change阶段的复杂度？
    view-change消息中的P包括checkpoint后每个请求的prepared状态证明（即N-f个prepare消息），复杂度为N；
    new-view消息中的V包括N-f个view-change消息，复杂度为N；
    new-view消息需要广播，复杂度为N。
    因此view-change阶段的复杂度为N^3^。

### HotStuff

- 如何将PBFT中O(N^3^)的view-change复杂度降低为O(N)?
PBFT中一个N用来验证消息是对的（primary发送的pre-prepare消息确实是view最高的且在某个replica处达到prepared状态的请求），一个N用来求消息并集（由于PBFT在一个view中可能完成多个请求的共识，每个replica达到了prepared状态的请求可能不同），HotStuff如何消除这两个N？
验证消息是对的可以用门限签名解决（这个论文中也提到了之前就有这种优化），求消息并集是直接用每个view只处理一个请求解决（这样不需要求并集，只要保留最高的，也就是highQC，但这也带来了下面一个问题）。
因此实际上，假设PBFT的一个view中，primary没有出错，并处理了m个请求，那么PBFT的1轮view-change对应HotStuff的m轮view-change，HotStuff的view-change复杂度劣化为O(mN)。

- 为什么相比于RBC、PBFT等协议，多了一个阶段？（**为什么需要**3PC）
核心原因就是primary的prepare消息中不含收集的new-view消息，因此有些东西无法证明，具体如下：
    - replica无法验证primary选择的是prepareQC是view最高的（由于为了降低view-change复杂度，primary发的prepare消息中不含收集的new-view消息）。
    - 需要replica已达到的prepareQC和当前primary发送的prepare不匹配时就拒绝（PBFT中replica达到prepared状态的消息与primary发送的pre-prepare不一致时，会接受primary的）（HotStuff中，如果不拒绝，可能存在，primary作恶，选择旧的分支，与已经commit的新的分支冲突，e.g.，v中正常提交，v+1中primary作恶，与f-1个faulty replica、f+1个non-faulty replica生成prepareQC，但故意使本轮共识超时，v+2正常提交，v+3时primary收集的new-view中一定包含v+2的prepareQC，但primary作恶，发送v+1的prepareQC，其他replica不应接受这个QC）。
    - 文章第9页*Livelessness with two-phases*这部分分析了，这种拒绝会导致可用性出现问题。
    ![HotStuff中的view-change机制](E:/研究生课程/笔记/Resource/HotStuff中的view-change机制.png)
    - 需要增加一个阶段（实际完成提交的是第三个阶段的commitQC、new-view中带的是第一个阶段的prepareQC、验证匹配的是第二个阶段的lockedQC）。

- 增加了一个阶段，是如何保证正确性的？（3PC具体是**如何解决问题**的）

    以下按一致性，可用性两方面进行分析。
    一致性：即假设某个请求在一个replica处完成提交（2PC的lockedQC或是3PC的commitQC），那么后续view的共识中有没有可能存在一种情况，使另一个冲突的请求（同序号但内容不同的请求）在某些replica处完成提交。
    可用性：即假设某个请求没有在任何一个replica处完成提交，那么后续view的共识中是否存在一种情况，使该请求一直不能在任何一个replica处完成提交。

    1. 2PC情况下
    - new-view中包含**prepareQC**，replica将primary发送的prepare消息中的highQC与自己的**prepareQC**比较。
    一致性：若某个请求在一个replica处达到lockedQC，则该请求至少在f+1个non-faulty replica处达到prepareQC，那么下一个view的主节点如果不选择这个prepareQC，至少会被f+1个non-faulty replica拒绝vote，无法产生冲突。
    可用性：见上一个问题的第三点。假设每个replica处prepareQC的view高度都不同，最坏情况下，下一个view的primary收到了最低的N-f个view的prepareQC，那么f个non-faulty replica会拒绝vote，如果同时f个faulty replica也拒绝vote，共识一直无法完成。
    也就是说，这种方案保证了一致性但不能保证可用性。
    - new-view中包含**prepareQC**，replica将primary发送的prepare消息中的highQC与自己的**lockedQC**比较。
    一致性：若某个请求在一个replica处达到lockedQC，但除了这个replica之外的所有replica可能都没有在当前view中对这个请求生成lockedQC，而下一个view的主节点如果不选择这个prepareQC，有可能只有这个replica拒绝vote，而其他replica都同意vote，产生了不一致。
    可用性：如果某个请求没有在任何一个replica处完成lockedQC，那么下一个view的primary发送任何一个该请求的prepareQC都是可以通过的，一致性没有问题。
    也就是说，这种方案保证了可用性但不能保证一致性。
    
    2. 3PC情况下
    这个就是HotStuff的最终解决方案，new-view中包含**prepareQC**，replica将primary发送的prepare消息中的highQC与自己的**lockedQC**比较，但共识的完成不是lockedQC，而是再下一个阶段的commitQC。
    一致性：若某个请求在一个replica处达到commitQC，则该请求至少在f+1个non-faulty replica处达到lockedQC和prepareQC，那么下一个view的主节点如果不选择这个prepareQC，至少会与f+1个non-faulty replica的lockedQC不一致，并被拒绝vote，无法产生冲突。相当于通过多一个阶段，保证non-faulty replica的lockedQC和prepareQC都能达到f+1，解决了2PC情况下第二个方案的一致性问题。
    可用性：与2PC情况下第二个方案相同。
    
    **note：** 该3PC方案的总体设计思路，首先因为view-change后primary发送的消息不包含highQC选取证据（怎么证明选择待共识的请求是最高view的？），需要replica去验证highQC并投票；然后发现replica用自己的prepareQC验证primary发送的prepareQC存在可用性问题，需要后一个阶段的QC（也就是lockedQC）去验证prepareQC；最后发现这种思路又带来一致性问题，需要再加一个阶段，也就是2PC变成了3PC。
